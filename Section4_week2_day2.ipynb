{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section4_week2_day2",
      "provenance": [],
      "collapsed_sections": [
        "Le-uGUKdzYSL"
      ],
      "authorship_tag": "ABX9TyMgytOT0a8YiWeAiwuBWDDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jungseunggi/Section4_week2/blob/main/Section4_week2_day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.Text Vectorization(텍스트 벡터화)**"
      ],
      "metadata": {
        "id": "C4NltnAoHpLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 단어 벡터를 이렇게 정하는 이유는 분포 가설(Distribution hypothesis)\n",
        "* **비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다**\n"
      ],
      "metadata": {
        "id": "Ue0AviBUJDdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 원핫 인코딩/벡터화(One-hot encoding/vector)**\n",
        "\n"
      ],
      "metadata": {
        "id": "ovOpb2P-JcrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 하나의 요소만 1이고 나머지는 모두 0인 희소 벡터(sparse vector)\n",
        "* 원-핫 벡터의 차원은 말뭉치(corpus) 내 단어의 수와 같음\n",
        "* 두 원-핫 벡터 간의 내적(inner product)이 0이다. 즉, 두 벡터는 직교(orthogonal)이며 서로 독립(independent)\n",
        "* 단어의 특정한 관계나 의미를 포함하지 않음\n",
        "* **따라서 cosine similarity를 사용 못함(내적하면 0이나오기 때문)**\n",
        "\n",
        "\n",
        "* 예시)  \n",
        "  \"you\", \"are\", \"not\", \"a\", \"smart\", \"student\"\n",
        "\n",
        "  - \"you\"     : 0\n",
        "  - \"are\"     : 1\n",
        "  - \"not\"     : 2\n",
        "  - \"a\"       : 3\n",
        "  - \"smart\"   : 4\n",
        "  - \"student\" : 5\n",
        "\n",
        "        \"You are a smart student.\"  \n",
        "    ---> [1, 1, 0, 1, 1, 1]"
      ],
      "metadata": {
        "id": "uNf7dDFHZCsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 단어 임베딩(Word Embedding)**\n"
      ],
      "metadata": {
        "id": "uz79Dek4MZZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 유사한 의미를 가진 단어가 유사한 표현을 가질 수 있도록 하는 단어 표현 유형\n",
        "* 개별 단어가 사전 정의된 **벡터 공간에서 실수 값 벡터로 표현되는 기술**\n",
        "* 접근 방식의 핵심은 각 단어에 대해 조밀한 분산 표현을 사용하는 아이디어\n",
        "* 원-핫 인코딩과 같은 희소 단어 표현에 필요한 수천 또는 수백만 차원과 대조"
      ],
      "metadata": {
        "id": "g7bxPi2OZFIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1 CBOW (Continuous Bag of Words) Embedding**"
      ],
      "metadata": {
        "id": "qiHAewWVyk1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 복수 단어 문맥(multi-word context)에 대한 문제  \n",
        "  즉, 여러개의 단어를 나열한 뒤 이와 관련된 단어를 추정하는 문제\n",
        "\n",
        "* 예를 들어,  \n",
        "      Betty bought a bit of better butter.\n",
        "\n",
        "  위 예시에서 (Betty, a bit, butter)라는 문맥이 주어지면 bought를 예측하는 구조\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/604/1*DfuBd49nCtT99h328iXL2Q.png\" width=\"300\">\n",
        "\n",
        "  <sub>이미지 출처: https://medium.com/@srishtee.kriti/mathematics-behind-continuous-bag-of-words-cbow-model-1e54cc2ecd88</sub>"
      ],
      "metadata": {
        "id": "_VU0DGKQtwoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2 Skip-Gram Embedding**"
      ],
      "metadata": {
        "id": "7be5_PAHt-QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CBOW 방식과 반대\n",
        "\n",
        "* 특정한 단어로부터 문맥이 될 수 있는 단어를 예측\n",
        "\n",
        "* 보통 입력 단어 주변의  $k$ 개 단어를 문맥으로 보고 예측 모형을 만드는데 이  $k$ 값을 window size 라고 함\n",
        "\n",
        "* 예시) Betty bought a bit of better butter / window size = 2 라면,\n",
        "        \n",
        "      Betty -> bought, butter  \n",
        "      bought -> butter, Betty  \n",
        "      a -> bit, of  \n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/publication/322905432/figure/fig1/AS:614314310373461@1523475353979/The-architecture-of-Skip-gram-model-20.png\" width=\"300\">\n",
        "\n",
        "  <sub> 이미지 출처: https://www.researchgate.net/figure/The-architecture-of-Skip-gram-model-20_fig1_322905432</sub>"
      ],
      "metadata": {
        "id": "7yTrBxqRuCFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3 Word2Vec**"
      ],
      "metadata": {
        "id": "16umkQ5LyNHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CBOW, Skip_Gram 방식의 단어 임베딩을 구현 \n",
        "  \n",
        "\n",
        "* subsampling, negative sampling 등의 기법 추가하여 학습 속도 향상\n",
        "\n",
        "  <img src=\"https://mbenhaddou.com/wp-content/uploads/2019/12/img_4.png\">\n",
        "\n",
        "  <sub>이미지 출처: http://mbenhaddou.com/2019/12/14/word2vec-concepts-from-scratch/</sub>\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/2456/1*gcC7b_v7OKWutYN1NAHyMQ.png\" width=\"800\">\n",
        "\n",
        "  <sub>이미지 출처: https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4</sub>"
      ],
      "metadata": {
        "id": "p3qPz6bmur7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##간단 예제"
      ],
      "metadata": {
        "id": "Le-uGUKdzYSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbQw30L1vU7M",
        "outputId": "e0ba0442-2d4b-46b1-e361-4567d8ef8e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "sentences = [list(s) for s in movie_reviews.sents()]"
      ],
      "metadata": {
        "id": "l-gK4jrOvdSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pHBJGfywcXg",
        "outputId": "251981e9-5326-4cc3-c445-e84e93f22dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plot',\n",
              " ':',\n",
              " 'two',\n",
              " 'teen',\n",
              " 'couples',\n",
              " 'go',\n",
              " 'to',\n",
              " 'a',\n",
              " 'church',\n",
              " 'party',\n",
              " ',',\n",
              " 'drink',\n",
              " 'and',\n",
              " 'then',\n",
              " 'drive',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "metadata": {
        "id": "BlUcqOnEwfQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences)"
      ],
      "metadata": {
        "id": "YXUdwaE8wlZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.init_sims(replace=True) # 모델 초기화 모델을 가져올때 메모리를 많이 써서"
      ],
      "metadata": {
        "id": "yifI3cqiwp1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity('fruit','apple') # 코사인 유사도 검사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD92Z_0TwxvH",
        "outputId": "f6381636-60f7-4a21-9b3a-95dddd561fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8239008"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('movie', topn=3) #가장 유사도 높은거"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWg5WMROxO3y",
        "outputId": "eac06175-b855-43a6-cbe1-0673b5ba2f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('film', 0.9377822875976562),\n",
              " ('picture', 0.8523768186569214),\n",
              " ('thing', 0.6947156190872192)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['car','sea'], negative = 'road', topn=2) # 긍적적인거에서 부정적인거 뺴주는거"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLUXBNJfxtQ6",
        "outputId": "ede8ce36-25c7-4908-e8cb-faf2a3fdf0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('room', 0.5630324482917786), ('intersect', 0.5579128861427307)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)"
      ],
      "metadata": {
        "id": "-GEnG-hpylie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_vocab = model.wv.vocab\n",
        "review_similarity = model[review_vocab]\n",
        "review_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2E1YZwywHz",
        "outputId": "4e78b680-ba3c-44da-e0aa-fa11ef0ad57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02813864, -0.00893146, -0.04626391, ...,  0.02355892,\n",
              "         0.08234709,  0.00818809],\n",
              "       [ 0.01720139, -0.04932543, -0.05642144, ...,  0.06605266,\n",
              "        -0.17390366, -0.26446798],\n",
              "       [-0.11967503, -0.01366821,  0.15134262, ..., -0.08689815,\n",
              "         0.10285499, -0.07813055],\n",
              "       ...,\n",
              "       [-0.01546955, -0.07243107, -0.080769  , ...,  0.2532195 ,\n",
              "        -0.02283047, -0.04276331],\n",
              "       [-0.01964428, -0.00480531, -0.01363833, ...,  0.24491291,\n",
              "        -0.00348324, -0.05845076],\n",
              "       [-0.0158069 , -0.00119361, -0.04649038, ...,  0.25870577,\n",
              "        -0.03255165, -0.07638636]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "similarity = tsne.fit_transform(review_similarity)\n",
        "review_df = pd."
      ],
      "metadata": {
        "id": "I0oq6j7zy8mi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}